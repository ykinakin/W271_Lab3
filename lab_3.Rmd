---
title: 'Lab 3: Panel Models'
subtitle: 'US Traffic Fatalities: 1980 - 2004'
output: 
  bookdown::pdf_document2: default
---

```{r load packages, echo=FALSE, message=FALSE}
library(plm)
library(stargazer)
library(tidyverse)
```


# U.S. traffic fatalities: 1980-2004

In this lab, we are asking you to answer the following **causal** question: 

> **"Do changes in traffic laws affect traffic fatalities?"**  

To answer this question, please complete the tasks specified below using the data provided in `data/driving.Rdata`. This data includes 25 years of data that cover changes in various state drunk driving, seat belt, and speed limit laws. 

Specifically, this data set contains data for the 48 continental U.S. states from 1980 through 2004. Various driving laws are indicated in the data set, such as the alcohol level at which drivers are considered legally intoxicated. There are also indicators for “per se” laws—where licenses can be revoked without a trial—and seat belt laws. A few economics and demographic variables are also included. The description of the each of the variables in the dataset is also provided in the dataset. 

```{r load data, echo = FALSE, message = FALSE}
load(file="./data/driving.RData")

## please comment these calls in your work 
# glimpse(data)
# desc
# 
# head(desc)
```

# (30 points, total) Build and Describe the Data 

1. (5 points) Load the data and produce useful features. Specifically: 

- Produce a new variable, called `speed_limit` that re-encodes the data that is in `sl55`, `sl65`, `sl70`, `sl75`, and `slnone`; 
    
```{r re-encode speed limit}
data[data$sl55 == 0.5, 4:6] <- 0
data[data$sl65 == 0.5, 5:6] <- 0

data_cleaned <- data %>%
  pivot_longer(col = sl55:slnone, names_to="speed_limit", names_prefix="sl") %>%
  filter(value >= 0.5) %>% 
  subset(select = -c(value))
```

- Produce a new variable, called `year_of_observation` that re-encodes the data that is in `d80`, `d81`, ... , `d04`.
    
```{r re-encode year}
data_cleaned <- data_cleaned[-c(which(colnames(data_cleaned)=="d80"):which(colnames(data_cleaned)=="d04"))]

data_cleaned <- data_cleaned %>% 
       dplyr::rename("year_of_observation" = "year")
```

- Produce a new variable for each of the other variables that are one-hot encoded (i.e. `bac*` variable series).

```{r re-encode bac}
data_cleaned <- add_column(data_cleaned, bacnone = 0, .after = "bac08")

data_cleaned <- data_cleaned %>% 
  mutate(
    bacnone = ifelse(bac10 == 0 & bac08 == 0, 1, 0),
    bac10 = ifelse(bac10 > 0 & bac08 == 0, 1, bac10)
    )

data_cleaned[data_cleaned$bac08 == 0.5, "bac10"] <- 0

data_cleaned <- data_cleaned %>%
  pivot_longer(col = bac10:bacnone, names_to="blood_alcohol_level", names_prefix="bac") %>%
  filter(value >= 0.5) %>%
  subset(select = -c(value))
```

- Rename these variables to sensible names that are legible to a reader of your analysis. For example, the dependent variable as provided is called, `totfatrte`. Pick something more sensible, like, `total_fatalities_rate`. There are few enough of these variables to change, that you should change them for all the variables in the data. (You will thank yourself later.)

```{r renaming cols}
col_name_lookup <- c(
  "min_drinking_age" = "minage",
  "zero_tol_law" = "zerotol",
  "grad_driver_law" = "gdl",
  "per_se_law" = "perse",
  "total_fatalities" = "totfat",
  "night_fatalities" = "nghtfat",
  "weekend_fatalities" = "wkndfat",
  "total_fatalities_per_100mil_miles" = "totfatpvm",
  "night_fatalities_per_100mil_miles" = "nghtfatpvm",
  "weekend_fatalities_per_100mil_miles" = "wkndfatpvm",
  "state_population" = "statepop",
  "total_fatalities_rate" = "totfatrte",
  "night_fatalities_rate" = "nghtfatrte",
  "weekend_fatalities_rate" = "wkndfatrte",
  "vehicle_miles" = "vehicmiles",
  "unemployment_rate" = "unem",
  "percent_age_14_24" = "perc14_24",
  "speed_limit_70_or_higher" = "sl70plus",
  "primary_seatbelt" = "sbprim",
  "secondary_seatbelt" = "sbsecon",
  "vehicle_miles_per_capita"="vehicmilespc"
  )

data_cleaned <- data_cleaned %>% 
  dplyr::rename(any_of(col_name_lookup))
```


2. (5 points) Provide a description of the basic structure of the dataset. What is this data? How, where, and when is it collected? Is the data generated through a survey or some other method? Is the data that is presented a sample from the population, or is it a *census* that represents the entire population? Minimally, this should include:
    - How is the our dependent variable of interest `total_fatalities_rate` defined? 
    
These data are complied by Dr. Donald G Freeman in his paper \textit{Drunk Driving Legislation and Traffic Fatalities: New Evidence on BAC 08 Laws}\footnote{https://doi.org/10.1111/j.1465-7287.2007.00039.x}, coming from several sources. Data on fatality rates are provided by the National Highway Traffic Safety Administration's (NHTSA) Fatality Analysis Reporting System (FARS). Data on traffic legislation for years between 1982 and 1999 provided by Thomas Dee, with data for later years downloaded directly from the NHTSA website. Unemployment data are taken from the US Bureau of Labour Statistics and age data are taken from the US Bureau of the Census. These data are not collected through a survey, but are rather sourced either directly from census or compiled by researchers or government agencies from publicly available information. These data should therefore be interpreted as census data that represent the entire population. 

The legislative variables provide information on the types of seat-belts laws in a jurisdiction, the legal limits for blood-alcohol levels for drivers, and the maximum speeds on state roads. Traffic fatality data are shown as raw count data, as well as ratios scaled either by miles driven or state population.  
    
```{r creating pdata}
pdriving <- pdata.frame(
  data_cleaned, 
  index=c("state", "year_of_observation")
  )

pdim(pdriving)

# Renaming States
replacement_state <- c("AL", "AZ", "AR", "CA", "CO", "CT", "DE", "FL", 
                       "GA", "ID", "IL", "IN", "IA", "KS", "KY", "LA", 
                       "ME", "MD", "MA", "MI", "MN", "MS", "MO", "MT", 
                       "NE", "NV", "NH", "NJ", "NM", "NY", "NC", "ND", 
                       "OH", "OK", "OR", "PA", "RI", "SC", "SD", "TN", 
                       "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY")

levels(pdriving$state) <- replacement_state

replacement_region <- c(
  "south", "southwest", "south", "west", "west", "northeast", "northeast", "south", "south",
  "west", "midwest", "midwest", "midwest", "midwest", "south", "south", "northeast", "northeast", 
  "northeast", "midwest", "midwest", "south", "midwest", "west", "midwest", "southwest", "northeast", 
  "northeast", "southwest", "northeast", "south", "midwest", "midwest", "southwest", "west", "northeast",
  "northeast", "south", "midwest", "south", "southwest", "west", "northeast", "south", "west", "south", 
  "midwest", "west")

pdriving$region <- pdriving$state
levels(pdriving$region) <- replacement_region
```


3. (20 points) Conduct a very thorough EDA, which should include both graphical and tabular techniques, on the dataset, including both the dependent variable `total_fatalities_rate` and the potential explanatory variables. Minimally, this should include: 
    - How is the our dependent variable of interest `total_fatalities_rate` defined? 
The response variable for the dataset, $total fatalities rate$, was generated by dividing the raw FARS traffic fatality counts by the state population normalized to 100,000 residents.
    - What is the average of `total_fatalities_rate` in each of the years in the time period covered in this dataset? 

```{r average of total fatalities}
pdriving %>%
  group_by(year_of_observation) %>%
  summarize(
    avg_fatality_rate = mean(total_fatalities_rate)
  )
```

```{r checking for NA}
sapply(pdriving, function(x)sum(is.na(x)))

table(pdriving$year_of_observation)

table(pdriving$state)

pdriving %>%
 is.pconsecutive()
```


```{r initial plots}
#pdriving %>%
#  group_by(state) %>%
#  ggplot(
#    aes(x = reorder(state,total_fatalities_rate), 
#        y = total_fatalities_rate)) +
#  geom_boxplot() +
#  labs(
#    x = "States",  
#    y = "Fatality rate"
#    )

pdriving %>%
  group_by(state) %>%
  ggplot(
    aes(x = reorder(state, total_fatalities_rate),
        y = total_fatalities_rate,
        colour = region)) +
  geom_boxplot() +
  labs(
    x = "State",
    y = "Fatality rate (traffic fatalities per 100k individuals)",
    ) +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)
  )

pdriving %>%
  group_by(year_of_observation) %>%
  ggplot(aes(x = year_of_observation, y = total_fatalities_rate)) +
  geom_boxplot() +
  labs(
    x = "States",  
    y = "Fatality rate"
    )

pdriving %>%
  group_by(state) %>%
  ggplot(aes(x = reorder(state,total_fatalities_rate), y = unemployment_rate)) +
  geom_boxplot() +
  labs(
    x = "States",  
    y = "Unemployment Rate",
    )

##############################################################

pdriving %>%
  ggplot(aes(x = log(state_population), y = total_fatalities_rate)) +
  geom_point(aes(colour = state)) +
  geom_line(aes(colour = state)) +
    labs(x = "Log(State Population)",  
       y = "Fatality rate (traffic fatalities per 100k individuals)", 
       ) +
  theme(
    legend.position = 'bottom'
  ) +
  guides(
    colour = guide_legend(nrow = 4)
  )

pdriving %>%
  ggplot(aes(x = percent_age_14_24, y = total_fatalities_rate)) +
  geom_point(aes(colour = state)) +
  geom_line(aes(colour = state)) +
    labs(x = "Percent of State Population aged 14-24",  
       y = "Fatality rate (traffic fatalities per 100k individuals)", 
       ) +
  theme(
    legend.position = 'bottom'
  ) +
  guides(
    colour = guide_legend(nrow = 4)
  )

pdriving %>%
  ggplot(aes(x = total_fatalities_per_100mil_miles, y = total_fatalities_rate)) +
  geom_point(aes(colour = state)) +
  geom_line(aes(colour = state)) +
    labs(x = "Fatality rate (traffic fatalities per 100M miles driven)", 
       y = "Fatality rate (traffic fatalities per 100k individuals)", 
       ) +
  theme(
    legend.position = 'bottom'
  ) +
  guides(
    colour = guide_legend(nrow = 4)
  )

#pdriving %>%
#  ggplot(aes(x = unemployment_rate, y = total_fatalities_rate)) +
#  geom_point(aes(colour = state)) +
#    labs(x = "Employment Rate", 
#       y = "Fatality rate (traffic fatalities per 100k individuals)", 
#       ) +
#  theme(
#    legend.position = 'bottom'
#  ) +
#  guides(
#    colour = guide_legend(nrow = 4)
#  )
```
From the initial plot, we can see that there is a range of fatality rates within each state over time. There is enough difference between many of states that there is little overlap in their upper and lower quartile ranges

We note that there is a range of fatality rates across states over the time period under consideration, with a general decrease in fatality rate from 1980 - 1990, after which the mean fatality rate across all states seems to remain relatively constant. The variance during all years under consideration looks generally stable, though as the mean moves towards zero, the distribution is increasingly skewed towards a fat tail. Of note, the northeast and mid-west regions of the country appear to in general have lower rates of driving fatalities, while those in the south, southwest and west generally have higher incidences. This may be attributable in some way to either the difference in climate or perhaps the state of the highway network. 

There appears to be a weak correlation between an increase in the percentage of the state's population aged 12-24 and increasing fatality rates. 

Interestingly, we note that an increase in a state's population is negatively correlated with the driving fatality rate. This effect is most prominent when looked at within state and does not apply more broadly (i.e. more populous states do not necessarily have fewer fatalities). This is not corrected for time, and one possible cause for this apparent correlation is that, as time goes on, state populations tend to increase while more restrictive driving laws also tend to be enacted and enforced (and cars tend to be better designed and more survivable during crashes). Somewhat paradoxically, the fatality rate per 100k individuals is positively correlated with the fatality rate per 100M miles driven. This is an exmaple of interactions between underlying variable causing spurious correlation. There is an obvious correlation between a state's population and the number of miles driven in that state, moderating the effect of increasing regulation decreasing fatality rate over time. 

No obvious correlation exists between total driving fatality rate and unemployment, either within state or globally across the dataset. 

As with every EDA this semester, the goal of this EDA is not to document your own process of discovery -- save that for an exploration notebook -- but instead it is to bring a reader that is new to the data to a full understanding of the important features of your data as quickly as possible. In order to do this, your EDA should include a detailed, orderly narrative description of what you want your reader to know. Do not include any output -- tables, plots, or statistics -- that you do not intend to write about.

# (15 points) Preliminary Model

Estimate a linear regression model of *totfatrte* on a set of dummy variables for the years 1981 through 2004 and interpret what you observe. In this section, you should address the following tasks: 

```{r Preliminary Model, echo = FALSE, results = 'latex'}
mod.prel <- plm(
  formula = total_fatalities_rate ~ year_of_observation, 
  data = pdriving,
  model = "pooling"
  )
stargazer(mod.prel, type = 'text')
```

```{r Preliminary Model Plot}
pdriving%>%
  ggplot(
    aes(
      x = year_of_observation, 
      y = total_fatalities_rate
      )
    ) +
  geom_point(
    color = "gray", 
    alpha = 0.4
    ) +
  geom_point(
    data = broom::augment(mod.prel),
    aes(x = year_of_observation, y = .fitted),
    colour = "blue", size = 3) +
  labs(
    x = "Year",
    y = "Fatality rate"
    ) +
  theme_classic() +
  theme(
  plot.title = element_text(color = "#0099F8",
                            size = 14,
                            face = "bold"),
  plot.subtitle = element_text(color="#969696",
                               size = 12,
                               face = "italic"),
  axis.title = element_text(color = "#969696",
                            size = 12,
                            face = "bold"),
  axis.text = element_text(color = "#969696", size = 10),
  axis.text.x = element_text(angle = 90),
  axis.line = element_line(color = "#969696"),
  axis.ticks = element_line(color = "#969696"),
  legend.position = "none"
  ) 
```

```{r Preliminary Model tests, echo = FALSE, results = 'latex'}
pcdtest(mod.prel, test ='lm')
pdwtest(mod.prel)
```

- Why is fitting a linear model a sensible starting place?
A linear model is always a sensible place to start as they are generally the most parsimoneous and easily explainable models. For this dataset in particular, in the EDA we have already identified some areas in which there appear to be linear relationships, therefore, it's entirely appropriate to start with linear methods in the modelling process. 
- What does this model explain, and what do you find in this model? 
This model suggests high significance for each of the years, with later years having more negative coefficients. This is expected as we previously noted that, on average, the fatality rate tends to decrease across states over time. 

- Did driving become safer over this period? Please provide a detailed explanation.

- What, if any, are the limitation of this model. In answering this, please consider **at least**: 
    - Are the parameter estimates reliable, unbiased estimates of the truth? Or, are they biased due to the way that the data is structured?
    - Are the uncertainty estimate reliable, unbiased estimates of sampling based variability? Or, are they biased due to the way that the data is structured? 

# (15 points) Expanded Model 

Expand the **Preliminary Model** by adding variables related to the following concepts: 

```{r}
mod.exp <- plm(
  formula = total_fatalities_rate ~ year_of_observation +
    blood_alcohol_level + per_se_law + primary_seatbelt +
    secondary_seatbelt + speed_limit_70_or_higher + grad_driver_law +
    percent_age_14_24 + unemployment_rate + vehicle_miles_per_capita, 
  data = pdriving,
  model = "pooling"
  )

summary(mod.exp)
```


```{r Expanded Model tests, echo = FALSE, results = 'latex'}
pcdtest(mod.exp, test ='lm')
pdwtest(mod.exp)
```

- Blood alcohol levels 
- Per se laws
- Primary seat belt laws (Note that if a law was enacted sometime within a year the fraction of the year is recorded in place of the zero-one indicator.)
- Secondary seat belt laws 
- Speed limits faster than 70 
- Graduated drivers licenses 
- Percent of the population between 14 and 24 years old
- Unemployment rate
- Vehicle miles driven per capita. 

If it is appropriate, include transformations of these variables. Please carefully explain carefully your rationale, which should be based on your EDA, behind any transformation you made. If no transformation is made, explain why transformation is not needed. 

- How are the blood alcohol variables defined? Interpret the coefficients that you estimate for this concept. 
- Do *per se laws* have a negative effect on the fatality rate? 
- Does having a primary seat belt law? 

# (15 points) State-Level Fixed Effects 

Re-estimate the **Expanded Model** using fixed effects at the state level. 

```{r Fixed Effect Model}
mod.fix <- plm(
  formula = total_fatalities_rate ~ state + year_of_observation +
    blood_alcohol_level + per_se_law + primary_seatbelt +
    secondary_seatbelt + speed_limit_70_or_higher + grad_driver_law +
    percent_age_14_24 + unemployment_rate + vehicle_miles_per_capita,
  data = pdriving,
  model = "within"
  )

summary(mod.fix)
```

```{r Fixed Model CI, echo = FALSE, results = 'latex'}
het_se <- sqrt(vcovHC(mod.fix, method="white1", type="HCO"))
bac_het_se <- het_se[33,33]
per_se_het_se <- het_se[26,26]
pri_sb_het_se <- het_se[25,25]
```

```{r Fixed Model tests, echo = FALSE, results = 'latex'}
pFtest(mod.fix, mod.exp)
```

- What do you estimate for coefficients on the blood alcohol variables? How do the coefficients on the blood alcohol variables change, if at all? 
- What do you estimate for coefficients on per se laws? How do the coefficients on per se laws change, if at all? 
- What do you estimate for coefficients on primary seat-belt laws? How do the coefficients on primary seatbelt laws change, if at all? 

Which set of estimates do you think is more reliable? Why do you think this? 

- What assumptions are needed in each of these models?  
- Are these assumptions reasonable in the current context?

# (10 points) Consider a Random Effects Model 

Instead of estimating a fixed effects model, should you have estimated a random effects model?

- Please state the assumptions of a random effects model, and evaluate whether these assumptions are met in the data. 
- If the assumptions are, in fact, met in the data, then estimate a random effects model and interpret the coefficients of this model. Comment on how, if at all, the estimates from this model have changed compared to the fixed effects model. 
- If the assumptions are **not** met, then do not estimate the data. But, also comment on what the consequences would be if you were to *inappropriately* estimate a random effects model. Would your coefficient estimates be biased or not? Would your standard error estimates be biased or not? Or, would there be some other problem that might arise?

```{r Random Effects Model}
mod.ran <- plm(
  formula = total_fatalities_rate ~ state + year_of_observation +
    blood_alcohol_level + per_se_law + primary_seatbelt +
    secondary_seatbelt + speed_limit_70_or_higher + grad_driver_law +
    percent_age_14_24 + unemployment_rate + vehicle_miles_per_capita,
  data = pdriving,
  model = "random"
  )

phtest(mod.fix, mod.ran)
```

# (10 points) Model Forecasts 

The COVID-19 pandemic dramatically changed patterns of driving. Find data (and include this data in your analysis, here) that includes some measure of vehicle miles driven in the US. Your data should at least cover the period from January 2018 to as current as possible. With this data, produce the following statements: 

- Comparing monthly miles driven in 2018 to the same months during the pandemic: 
  - What month demonstrated the largest decrease in driving? How much, in percentage terms, lower was this driving? 
  - What month demonstrated the largest increase in driving? How much, in percentage terms, higher was this driving? 
  
Now, use these changes in driving to make forecasts from your models. 

- Suppose that the number of miles driven per capita, increased by as much as the COVID boom. Using the FE estimates, what would the consequences be on the number of traffic fatalities? Please interpret the estimate.
- Suppose that the number of miles driven per capita, decreased by as much as the COVID bust. Using the FE estimates, what would the consequences be on the number of traffic fatalities? Please interpret the estimate.

# (5 points) Evaluate Error 

If there were serial correlation or heteroskedasticity in the idiosyncratic errors of the model, what would be the consequences on the estimators and their standard errors? Is there any serial correlation or heteroskedasticity? 


```{r Best Model tests, echo = FALSE, results = 'latex'}
pcdtest(mod.fix, test ='lm')
pdwtest(mod.fix)
pbgtest(mod.fix, order=2)
```

```{r Best Model errors, echo = FALSE, results = 'latex'}
reg.se <- coef(summary(mod.fix))[1,2]
het.se <- sqrt(vcovHC(mod.fix, method="white1", type="HC0")[1,1])
cluster.se <- sqrt(vcovHC(mod.fix, method="white2", type="HC0")[1,1])
nw.se <- sqrt(vcovNW(mod.fix, type="HC0", maxlag=1)[1,1])
arellano.se <- sqrt(vcovHC(mod.fix, method="arellano", type="HC0")[1,1])
knitr::kable(data.frame(
  "Type" = c("Regular OLS", "Robust", "Cluster Robust", "Newey West", "Arellano"),
  "SE" = c(reg.se, het.se, cluster.se, nw.se, arellano.se)
), "latex")

```
