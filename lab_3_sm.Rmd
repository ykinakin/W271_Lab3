---
title: 'Lab 3: Panel Models'
subtitle: 'US Traffic Fatalities: 1980 - 2004'
output: 
  bookdown::pdf_document2: default
---

```{r load packages, echo=FALSE, message=FALSE}
library(tidyverse)
library(plm)
library(stargazer)
```


# U.S. traffic fatalities: 1980-2004

In this lab, we are asking you to answer the following **causal** question: 

> **"Do changes in traffic laws affect traffic fatalities?"**  

To answer this question, please complete the tasks specified below using the data provided in `data/driving.Rdata`. This data includes 25 years of data that cover changes in various state drunk driving, seat belt, and speed limit laws. 

Specifically, this data set contains data for the 48 continental U.S. states from 1980 through 2004. Various driving laws are indicated in the data set, such as the alcohol level at which drivers are considered legally intoxicated. There are also indicators for “per se” laws—where licenses can be revoked without a trial—and seat belt laws. A few economics and demographic variables are also included. The description of the each of the variables in the dataset is also provided in the dataset. 

```{r load data, echo = TRUE}
load(file="./data/driving.RData")

## please comment these calls in your work 
# glimpse(data)
# desc
# 
# head(desc)
```

# (30 points, total) Build and Describe the Data 

1. (5 points) Load the data and produce useful features. Specifically: 

- Produce a new variable, called `speed_limit` that re-encodes the data that is in `sl55`, `sl65`, `sl70`, `sl75`, and `slnone`; 
    
```{r re-encode speed limit}
data[data$sl55 == 0.5, 4:6] <- 0
data[data$sl65 == 0.5, 5:6] <- 0

data_cleaned <- data %>%
  pivot_longer(col = sl55:slnone, names_to="speed_limit", names_prefix="sl") %>%
  filter(value >= 0.5) %>% 
  subset(select = -c(value))
```

- Produce a new variable, called `year_of_observation` that re-encodes the data that is in `d80`, `d81`, ... , `d04`.
    
```{r re-encode year}
data_cleaned <- data_cleaned[-c(which(colnames(data_cleaned)=="d80"):which(colnames(data_cleaned)=="d04"))]

data_cleaned <- data_cleaned %>% 
       rename("year_of_observation" = "year")
```

- Produce a new variable for each of the other variables that are one-hot encoded (i.e. `bac*` variable series).

```{r re-encode bac}
data_cleaned <- add_column(data_cleaned, bacnone = 0, .after = "bac08")

data_cleaned <- data_cleaned %>% 
  mutate(
    bacnone = ifelse(bac10 == 0 & bac08 == 0, 1, 0),
    bac10 = ifelse(bac10 > 0 & bac08 == 0, 1, bac10)
    )

data_cleaned[data_cleaned$bac08 == 0.5, "bac10"] <- 0

data_cleaned <- data_cleaned %>%
  pivot_longer(col = bac10:bacnone, names_to="blood_alcohol_level", names_prefix="bac") %>%
  filter(value >= 0.5) %>%
  subset(select = -c(value))
```

- Rename these variables to sensible names that are legible to a reader of your analysis. For example, the dependent variable as provided is called, `totfatrte`. Pick something more sensible, like, `total_fatalities_rate`. There are few enough of these variables to change, that you should change them for all the variables in the data. (You will thank yourself later.)

```{r renaming cols}
col_name_lookup <- c(
  "min_drinking_age" = "minage",
  "zero_tol_law" = "zerotol",
  "grad_driver_law" = "gdl",
  "per_se_law" = "perse",
  "total_fatalities" = "totfat",
  "night_fatalities" = "nghtfat",
  "weekend_fatalities" = "wkndfat",
  "total_fatalities_per_100mil_miles" = "totfatpvm",
  "night_fatalities_per_100mil_miles" = "nghtfatpvm",
  "weekend_fatalities_per_100mil_miles" = "wkndfatpvm",
  "state_population" = "statepop",
  "total_fatalities_rate" = "totfatrte",
  "night_fatalities_rate" = "nghtfatrte",
  "weekend_fatalities_rate" = "wkndfatrte",
  "vehicle_miles" = "vehicmiles",
  "unemployment_rate" = "unem",
  "percent_age_14_24" = "perc14_24",
  "speed_limit_70_or_higher" = "sl70plus",
  "primary_seatbelt" = "sbprim",
  "secondary_seatbelt" = "sbsecon",
  "vehicle_miles_per_capita"="vehicmilespc"
  )

data_cleaned <- data_cleaned %>% 
  rename(any_of(col_name_lookup))
```


2. (5 points) Provide a description of the basic structure of the dataset. What is this data? How, where, and when is it collected? Is the data generated through a survey or some other method? Is the data that is presented a sample from the population, or is it a *census* that represents the entire population? Minimally, this should include:
    - How is the our dependent variable of interest `total_fatalities_rate` defined? 
    
```{r creating pdata}
pdriving <- pdata.frame(
  data_cleaned, 
  index=c("state", "year_of_observation")
  )

pdim(pdriving)

# Renaming States
replacement_state <- c("AL", "AZ", "AR", "CA", "CO", "CT", "DE", "FL", 
                       "GA", "ID", "IL", "IN", "IA", "KS", "KY", "LA", 
                       "ME", "MD", "MA", "MI", "MN", "MS", "MO", "MT", 
                       "NE", "NV", "NH", "NJ", "NM", "NY", "NC", "ND", 
                       "OH", "OK", "OR", "PA", "RI", "SC", "SD", "TN", 
                       "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY")

levels(pdriving$state) <- replacement_state

replacement_region <- c(
  "south", "southwest", "south", "west", "west", "northeast", "northeast", "south", "south",
  "west", "midwest", "midwest", "midwest", "midwest", "south", "south", "northeast", "northeast", 
  "northeast", "midwest", "midwest", "south", "midwest", "west", "midwest", "southwest", "northeast", 
  "northeast", "southwest", "northeast", "south", "midwest", "midwest", "southwest", "west", "northeast",
  "northeast", "south", "midwest", "south", "southwest", "west", "northeast", "south", "west", "south", 
  "midwest", "west")

pdriving$region <- pdriving$state
levels(pdriving$region) <- replacement_region
```

> *Provide a description of the basic structure of the dataset*

> The variables of the dataset are summarized here: https://cran.r-project.org/web/packages/wooldridge/wooldridge.pdf

> After cleaning the data, the panel dataset has 27 columns and 1,200 rows. With each year from 1980 to 2004 having 48 rows for each of the lower 48 states (i.e., excluding Alaska and Hawaii). For each year and for each state information is given about the total number of fatalities per 100,000 people under several different criteria (e.g., number that occured at night or on the weekend), as well as certain demographic data such as unemployment rate, characteristics of the individuals and cars in the fatalities, and various laws for each state in each year about blood alcohol level and whether or not a seatbelt is required.

> *What is the data?*

> The data is collecting the total fatalities per 100,000 in each state, as well as several other potentially significant features.

> *How, where, and when is it collected?*

> The data is collected for each of the continental 48 states for each year from 1980 to 2004. The dependent variable of interest is the total fatalities per 100,000 population in each state. It's unclear how population is determined, if it is for the population as a whole, population of drivers only, another metric. 

> *Is the data generated through a survey or some other method?*

> It's unclear how exactly the data was collected. Presumably the data comes from a government source, but it is standardized to be the number of fatalities per 100,000 of the population. It's assumed that total fatalities are the number of total people who passed, and not the number of fatal accidents alone. For example, if three people passed, it's assumed all three individuals are included in the total fatalities variable. 

> *Is the data that is presented a sample from the population, or is it a census that represents the entire population?*

> The data is a sample of the population, of the total number of fatalities per 100,000. 

> *How is the dependent variable of interest defined*

> The dataset defines "totfatrte" as "total fatalities per 100,000 population"


3. (20 points) Conduct a very thorough EDA, which should include both graphical and tabular techniques, on the dataset, including both the dependent variable `total_fatalities_rate` and the potential explanatory variables. Minimally, this should include: 
    - How is the our dependent variable of interest `total_fatalities_rate` defined? 
    
    > The dependent variable of interest is defined above, but it is the total number of fatalities per 100,000 of the population.
    
    - What is the average of `total_fatalities_rate` in each of the years in the time period covered in this dataset? 
    
```{r}
summary_by_year <- pdriving %>%
  select(year_of_observation, total_fatalities_rate) %>%
  group_by(year_of_observation) %>%
  summarize(avg = mean(total_fatalities_rate))

ggplot(summary_by_year, aes(x=year_of_observation, y=avg))+
  geom_bar(stat="identity")+
  labs(
      title = "Average Fatalities per 100,000 by Year",
      x = "Year",
      y = "Average Fatalities per 100,000"
    ) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```


    
    > The graph above shows the average falities per 100,000 from 1980 to 2004. From the graph we can see that the fatalities decline from 1980 to 1985 before having a few years of increase from 1985 to 1988. The rates than decline from 1988 to 1995 where they appear to stabalize. 

```{r checking for NA}
sapply(pdriving, function(x)sum(is.na(x)))

table(pdriving$year_of_observation)

table(pdriving$state)

pdriving %>%
 is.pconsecutive()
```


```{r initial plots}
pdriving %>%
  group_by(state) %>%
  ggplot(
    aes(x = reorder(state,total_fatalities_rate), 
        y = total_fatalities_rate)) +
  geom_boxplot() +
  labs(
    x = "States",  
    y = "Fatality rate"
    )

pdriving %>%
  group_by(year_of_observation) %>%
  ggplot(aes(x = year_of_observation, y = total_fatalities_rate)) +
  geom_boxplot() +
  labs(
    x = "States",  
    y = "Fatality rate"
    )

pdriving %>%
  group_by(state) %>%
  ggplot(aes(x = reorder(state,total_fatalities_rate), y = unemployment_rate)) +
  geom_boxplot() +
  labs(
    x = "States",  
    y = "Unemployment Rate",
    )

##############################################################
pdriving %>%
  group_by(state) %>%
  ggplot(
    aes(x = reorder(state, total_fatalities_rate),
        y = total_fatalities_rate,
        colour = region)) +
  geom_boxplot() +
  labs(
    x = "State",
    y = "Fatality rate (traffic fatalities per 100k individuals)",
    ) +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)
  )

pdriving %>%
  ggplot(aes(x = log(state_population), y = total_fatalities_rate)) +
  geom_point(aes(colour = state)) +
  geom_line(aes(colour = state)) +
    labs(x = "Log(State Population)",  
       y = "Fatality rate (traffic fatalities per 100k individuals)", 
       ) +
  theme(
    legend.position = 'bottom'
  ) +
  guides(
    colour = guide_legend(nrow = 4)
  )

pdriving %>%
  ggplot(aes(x = percent_age_14_24, y = total_fatalities_rate)) +
  geom_point(aes(colour = state)) +
  geom_line(aes(colour = state)) +
    labs(x = "Percent of State Population aged 14-24",  
       y = "Fatality rate (traffic fatalities per 100k individuals)", 
       ) +
  theme(
    legend.position = 'bottom'
  ) +
  guides(
    colour = guide_legend(nrow = 4)
  )

pdriving %>%
  ggplot(aes(x = total_fatalities_per_100mil_miles, y = total_fatalities_rate)) +
  geom_point(aes(colour = state)) +
  geom_line(aes(colour = state)) +
    labs(x = "Fatality rate (traffic fatalities per 100M miles driven)", 
       y = "Fatality rate (traffic fatalities per 100k individuals)", 
       ) +
  theme(
    legend.position = 'bottom'
  ) +
  guides(
    colour = guide_legend(nrow = 4)
  )

pdriving %>%
  ggplot(aes(x = unemployment_rate, y = total_fatalities_rate)) +
  geom_point(aes(colour = state)) +
    labs(x = "Employment Rate", 
       y = "Fatality rate (traffic fatalities per 100k individuals)", 
       ) +
  theme(
    legend.position = 'bottom'
  ) +
  guides(
    colour = guide_legend(nrow = 4)
  )
```
> We note that there is a range of fatality rates across states over the time period under consideration, with a general decrease in fatality rate from 1980 - 1990, after which the mean fatality rate across all states seems to remain relatively constant. The variance during all years under consideration looks generally stable, though as the mean moves towards zero, the distribution is increasingly skewed towards a fat tail. Interestingly, the northeast and mid-west regions of the country appear to in general have lower rates of driving fatalities, while those in the south, southwest and west generally have higher incidences. This may be attributable in some way to either the difference in climate or perhaps the state of the highway network. 

> There appears to be a weak correlation between an increase in the percentage of the state's population aged 12-24 and increasing fatality rates. 

> Interestingly, we note that an increase in a state's population is negatively correlated with the driving fatality rate. This effect is most prominent when looked at within state and does not apply more broadly (i.e. more populous states do not necessarily have fewer fatalities). This is not corrected for time, and one possible cause for this apparent correlation is that, as time goes on, state populations tend to increase while more restrictive driving laws also tend to be enacted and enforced (and cars tend to be better designed and more survivable during crashes). Somewhat paradoxically, the fatality rate per 100k individuals is positively correlated with the fatality rate per 100M miles driven. This is an exmaple of interactions between underlying variable causing spurious correlation. There is an obvious correlation between a state's population and the number of miles driven in that state, moderating the effect of increasing regulation decreasing fatality rate over time. 

> No obvious correlation exists between total driving fatality rate and unemployment, either within state or globally across the dataset. 


```{r}
summary(data_cleaned)
```


```{r numeric histograms}
#Histograms for Total Fatalities
#ggplot(data_cleaned, aes(x = total_fatalities)) + geom_histogram()
#ggplot(data_cleaned, aes(x = log(total_fatalities))) + geom_histogram()

#Histograms for Fatalities per 100miles
#ggplot(data_cleaned, aes(x = total_fatalities_per_100mil_miles)) + geom_histogram()
#ggplot(data_cleaned, aes(x = log(total_fatalities_per_100mil_miles))) + geom_histogram()

#Histograms for state population
ggplot(data_cleaned, aes(x = state_population)) + geom_histogram()
ggplot(data_cleaned, aes(x = log(state_population))) + geom_histogram()

#Histograms for total fatalities rate
ggplot(data_cleaned, aes(x = total_fatalities_rate)) + geom_histogram()
ggplot(data_cleaned, aes(x = log(total_fatalities_rate))) + geom_histogram()

#Histograms for vehicle miles
ggplot(data_cleaned, aes(x = vehicle_miles)) + geom_histogram()
ggplot(data_cleaned, aes(x = log(vehicle_miles))) + geom_histogram()

#Histograms for unemployment rate
ggplot(data_cleaned, aes(x = unemployment_rate)) + geom_histogram()
ggplot(data_cleaned, aes(x = log(unemployment_rate))) + geom_histogram()

#Histograms for vehicle miles per capita
ggplot(data_cleaned, aes(x = vehicle_miles_per_capita)) + geom_histogram()
ggplot(data_cleaned, aes(x = log(vehicle_miles_per_capita))) + geom_histogram()

```

```{r bivariate analysis}
#should we convert this to a factor?

ggplot(data_cleaned, aes(x = as.factor(zero_tol_law), y=total_fatalities_rate)) + geom_boxplot()
ggplot(data_cleaned, aes(x = as.factor(grad_driver_law), y=total_fatalities_rate)) + geom_boxplot()
ggplot(data_cleaned, aes(x = as.factor(per_se_law), y=total_fatalities_rate)) + geom_boxplot()
ggplot(data_cleaned, aes(x = as.factor(speed_limit_70_or_higher), y=total_fatalities_rate)) + geom_boxplot()
ggplot(data_cleaned, aes(x = as.factor(primary_seatbelt), y=total_fatalities_rate)) + geom_boxplot()
ggplot(data_cleaned, aes(x = as.factor(secondary_seatbelt), y=total_fatalities_rate)) + geom_boxplot()
ggplot(data_cleaned, aes(x = as.factor(blood_alcohol_level), y=total_fatalities_rate)) + geom_boxplot()

```

```{r}

```


# (15 points) Preliminary Model

Estimate a linear regression model of *totfatrte* on a set of dummy variables for the years 1981 through 2004 and interpret what you observe. In this section, you should address the following tasks: 

```{r Preliminary Model, warning=FALSE}
mod.prel <- lm(
  formula = total_fatalities_rate ~ as.factor(year_of_observation),
  data = data_cleaned
  )

stargazer(mod.prel, type = 'text')
```
```{r Preliminary Model Plot}
#data formatting
projected <- broom::augment(mod.prel)
projected <- projected %>%
  rename(
    year_of_observation = "as.factor(year_of_observation)"
  )
projected$year_of_observation <- as.numeric(projected$year_of_observation) + 1979

#graph
data_cleaned %>%
  ggplot(
    aes(
      x = year_of_observation, 
      y = total_fatalities_rate
      )
    ) +
  geom_point(
    color = "gray", 
    alpha = 0.4
    ) +
  geom_point(
    data = projected,
    aes(x = year_of_observation, y = .fitted),
    colour = "blue", size = 3) +
  labs(
    x = "Year",
    y = "Fatality rate",
    title = "Average Fatality Rate by Year"
    ) +
  theme_classic() +
  theme(
  plot.title = element_text(color = "#0099F8",
                            size = 14,
                            face = "bold"),
  plot.subtitle = element_text(color="#969696",
                               size = 12,
                               face = "italic"),
  axis.title = element_text(color = "#969696",
                            size = 12,
                            face = "bold"),
  axis.text = element_text(color = "#969696", size = 10),
  axis.text.x = element_text(angle = 90),
  axis.line = element_line(color = "#969696"),
  axis.ticks = element_line(color = "#969696"),
  legend.position = "none"
  ) 
```

- Why is fitting a linear model a sensible starting place?

> A linear model is always a sensible place to start as they are generally the most parsimoneous and easily explainable models. For this dataset in particular, in the EDA we have already identified some areas in which there appear to be linear relationships, therefore, it's entirely appropriate to start with linear methods in the modelling process. 

- What does this model explain, and what do you find in this model? 

> The linear model is above is only a function of year. Where each year is predicting the average. As we can see, that as year increases there is a slight deline in the total fatalities rate, and eventually begins to stabalize. 

- Did driving become safer over this period? Please provide a detailed explanation.

> While the overall fatalities rate has declined We can't say that driving has become safer over the period, as we can see see a significant variance in the results versus the projection. There may be missing variables that are causing the decrease in fatalities.

- What, if any, are the limitation of this model. In answering this, please consider **at least**: 
    - Are the parameter estimates reliable, unbiased estimates of the truth? Or, are they biased due to the way that the data is structured?
    
    > The only variable is for year, which is not necessarily unbiased estimator of the truth. There is omitted variable risk, and we need to explore alternative models which may be better predictors of total fatalities than just time alone. 
    
    - Are the uncertainty estimate reliable, unbiased estimates of sampling based variability? Or, are they biased due to the way that the data is structured? 
    
    > Since the total fatalities can potentially differ by the states due to their laws and safety requirements, there is a potential bias in the data if these features aren't considered within the final model.  

# (15 points) Expanded Model 

Expand the **Preliminary Model** by adding variables related to the following concepts: 

- Blood alcohol levels 
- Per se laws
- Primary seat belt laws (Note that if a law was enacted sometime within a year the fraction of the year is recorded in place of the zero-one indicator.)
- Secondary seat belt laws 
- Speed limits faster than 70 
- Graduated drivers licenses 
- Percent of the population between 14 and 24 years old
- Unemployment rate
- Vehicle miles driven per capita. 

If it is appropriate, include transformations of these variables. Please carefully explain carefully your rationale, which should be based on your EDA, behind any transformation you made. If no transformation is made, explain why transformation is not needed. 

```{r expanded model, warning=FALSE}
mod.exp <- lm(
  formula = total_fatalities_rate ~ as.factor(year_of_observation) +
    blood_alcohol_level + per_se_law + primary_seatbelt +
    secondary_seatbelt + speed_limit_70_or_higher + grad_driver_law +
    percent_age_14_24 + unemployment_rate + vehicle_miles_per_capita, 
  data = data_cleaned
  )

stargazer(mod.exp, type = 'text')
```

- Transformation of variables

> To be entered. Need to describe hw we changed between the year. 

- How are the blood alcohol variables defined? Interpret the coefficients that you estimate for this concept. 

> "bac10" means there is a blood alcohol limit of 0.10, and "bac08" means there is a blood alcohol limit of 0.08 to be considered legally intoxicated. When neither bac10 or bac08 was flagged, we created a new indicator for "none".

> The baseline factor is a blood alcohol limit of 0.08, and we can see that when the the limit increases to .10, the number of casualties increases by 0.06. However, the increase amount is not highly significant.

> However, the change between a blood alcohol limit of 0.08 and none, increases the total fatalities rate by 2 and this increase is highly significant. 

- Do *per se laws* have a negative effect on the fatality rate? 

> When per se laws (i.e. administrative license revocation) are present the total fatality rate decreases by 0.8, and this is statistically significant at 0.05.

- Does having a primary seat belt law? 

> Primary seatbelt laws are shown to increase the total number of fatalities, however, the results are not significant even at the 0.1 level. 

# (15 points) State-Level Fixed Effects 

Re-estimate the **Expanded Model** using fixed effects at the state level. 

```{r Fixed Effect Model, warning=FALSE}
mod.within <- plm(
  formula = total_fatalities_rate ~ state + year_of_observation + #Do we even need to put state in here?
    blood_alcohol_level + per_se_law + primary_seatbelt +
    secondary_seatbelt + speed_limit_70_or_higher + grad_driver_law +
    percent_age_14_24 + unemployment_rate + vehicle_miles_per_capita,
  data = pdriving, #indexed on state and year
  model = "within" #adjusting to a fixed effect model
  )

mod.fd <- plm(
  formula = total_fatalities_rate ~ state + year_of_observation + #Do we even need to put state in here?
    blood_alcohol_level + per_se_law + primary_seatbelt +
    secondary_seatbelt + speed_limit_70_or_higher + grad_driver_law +
    percent_age_14_24 + unemployment_rate + vehicle_miles_per_capita,
  data = pdriving, #indexed on state and year
  model = "within" #adjusting to a fixed effect model
  )

stargazer(mod.fix, type = 'text')

mod.fix <- plm(
  formula = total_fatalities_rate ~ state + year_of_observation + #Do we even need to put state in here?
    blood_alcohol_level + per_se_law + primary_seatbelt +
    secondary_seatbelt + speed_limit_70_or_higher + grad_driver_law +
    percent_age_14_24 + unemployment_rate + vehicle_miles_per_capita,
  data = pdriving, #indexed on state and year
  model = "within" #adjusting to a fixed effect model
  )

stargazer(mod.fix, type = 'text')
```

- What do you estimate for coefficients on the blood alcohol variables? How do the coefficients on the blood alcohol variables change, if at all? 

> The variables for blood alcohol change as follows:

| Model        | Level 0.1   | Level None   |
|--------------|-------------|--------------|
| Pooled       | 1.053       | 2.279        |
| Fixed Effect | 0.27426     | 1.1165       |

> As we can see that when adding the state level factors the coefficients for blood alcohol levels change signficantly. For example, when switching from 0.08 to 0.1, when accounting for states, will decrease (rather than increase) the total fatality rate, which seems more intuitively correct. While the impact of not having a level specified is dampened, reducing from 2 to 0.7.

- What do you estimate for coefficients on per se laws? How do the coefficients on per se laws change, if at all? 

> The coefficient on per se laws changes as follows:

| Model        | Per Se Law  |
|--------------|-------------|
| Pooled       | -.6524     |
| Fixed Effect | -1.1874     |

> Here we can see a stronger impact of implementing per se laws, with the reduction in fatalities going down by 1.5 vs. 0.8 in the original model. 

- What do you estimate for coefficients on primary seat-belt laws? How do the coefficients on primary seatbelt laws change, if at all? 

> The coefficnet on seatbelt laws changes as follows:

| Model        | Primary     | Secondary    |
|--------------|-------------|--------------|
| Pooled       | -0.08188    | 0.06808      |
| Fixed Effect | -1.2254     | -3.4882      |

> Note the secondary seatbelt is not statistically significant for either model, and the primary seatbelt is only significant in the model where states are added.

> Here we can see that by implementing seatbelt laws there is a decrease in the total fatality rate, which is more consistent with our natural understanding.

Which set of estimates do you think is more reliable? Why do you think this? 

> Between the two models, we would consider the model that includes a variable for each state to be more reliable. Several of the variables are statistically significant, which indicates they're potentially important to include, and the direction of coefficients on other variables seems more logical.

- What assumptions are needed in each of these models?  

# CHeck with Mark what level of assumptions are needed. Is it simply a covariance test. 
  
  > For a fixed effect model, the necessary assumptions are as follows (taken directly from the live session):
  
  > 1. For each $i$, the model is:
  
  $$ y_{it} = \beta_1x_{it1} + ... + \beta_kx_{itk} + a_i + u_{it}, t = 1,...,T$$
  > where the $\beta_j$ are the parameters to estimate and $a_i$ is the unobserved effect.
  
  > 2. We have a random sample from the cross section
  > 3. Each explanatory variable changes over time (for at least some i), and no perfect linear relationships exist among the explanatory variables.
  > 4. For each $t$, the expected value of the idiosyncratic error given the explanatory variables in all time periods and the unobserved effect si zero: $E(u_{it}|X_i,a_i) = 0$.
  > 5. For all t = 1,...,T:
  
  $$Var(u_{it} | X_i,a_i) = Var(u_{it}) = \sigma^2_\mu$$
  
  > 6. for all $t \neq s$, the idiosyncratic errors are uncorrelated (conditional on all explanatory variables and a_i): $Cov(u_{it},u_{is}|X_i,a_i) = 0$
  
- Are these assumptions reasonable in the current context?

  > To be entered

# (10 points) Consider a Random Effects Model 

Instead of estimating a fixed effects model, should you have estimated a random effects model?

- Please state the assumptions of a random effects model, and evaluate whether these assumptions are met in the data. 

> In addition to the fixed effect assumptions #1, #2, #4, #5 and #6, there are the three additional assumptions for a random effect model are: 

> 1. There are no perfect linear relationships among the explanatory variables.
> 2. The expected value of $a_i$ given all explanatory variables is constant: $E(a_i | X_i) = \beta_0$
> 3. The variance of $a_i$ given all explanatory variables is constant: $Var(a_i | X_i) = \sigma^2_a$

- If the assumptions are, in fact, met in the data, then estimate a random effects model and interpret the coefficients of this model. Comment on how, if at all, the estimates from this model have changed compared to the fixed effects model. 

```{r Random Effect Model, warning=FALSE}

mod.random <- plm(
  formula = total_fatalities_rate ~ year_of_observation +
    blood_alcohol_level + per_se_law + primary_seatbelt +
    secondary_seatbelt + speed_limit_70_or_higher + grad_driver_law +
    percent_age_14_24 + unemployment_rate + vehicle_miles_per_capita ,
  data = pdriving, #indexed on state and year
  model = "random" #adjusting to a random effect model
  )

stargazer(mod.random, type = 'text')

```


- If the assumptions are **not** met, then do not estimate the data. But, also comment on what the consequences would be if you were to *inappropriately* estimate a random effects model. Would your coefficient estimates be biased or not? Would your standard error estimates be biased or not? Or, would there be some other problem that might arise?
```{r}
phtest(mod.fix, mod.random)
```



# (10 points) Model Forecasts 

The COVID-19 pandemic dramatically changed patterns of driving. Find data (and include this data in your analysis, here) that includes some measure of vehicle miles driven in the US. Your data should at least cover the period from January 2018 to as current as possible. With this data, produce the following statements: 

```{r}
#https://www.fhwa.dot.gov/policyinformation/travel_monitoring/tvt.cfm


```


- Comparing monthly miles driven in 2018 to the same months during the pandemic: 
  - What month demonstrated the largest decrease in driving? How much, in percentage terms, lower was this driving? 
  - What month demonstrated the largest increase in driving? How much, in percentage terms, higher was this driving? 
  
Now, use these changes in driving to make forecasts from your models. 

- Suppose that the number of miles driven per capita, increased by as much as the COVID boom. Using the FE estimates, what would the consequences be on the number of traffic fatalities? Please interpret the estimate.
- Suppose that the number of miles driven per capita, decreased by as much as the COVID bust. Using the FE estimates, what would the consequences be on the number of traffic fatalities? Please interpret the estimate.

# (5 points) Evaluate Error 

If there were serial correlation or heteroskedasticity in the idiosyncratic errors of the model, what would be the consequences on the estimators and their standard errors? Is there any serial correlation or heteroskedasticity? 